{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules from the commutazzio library and standard Python libraries\n",
    "from commutazzio.random import RandomFiltrationPointCloudModel\n",
    "from commutazzio.filtration import CLFiltrationDB\n",
    "from commutazzio.plot import ComplementaryTrianglesPlot as Visualizer1\n",
    "import random\n",
    "import numpy as np\n",
    "from commutazzio.utils import filepath_generator\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import yaml\n",
    "# Loading model parameters from YAML configuration file\n",
    "with open('./model_parameters.yaml', 'r') as file:\n",
    "    model_parameters = yaml.safe_load(file)\n",
    "POINT_CLOUD_PARAMS = model_parameters['point_cloud']\n",
    "# Setting up database name and directory\n",
    "DB_NAME = 'PointCloudModel_tutorial'\n",
    "DIR_NAME = 'tutorial_databases'\n",
    "# Creating the tutorial databases directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(DIR_NAME):\n",
    "    os.makedirs(DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and analyze filtrations for a specified number of Point Cloud Model instances\n",
    "def generate_filtration_point_cloud_model(num_instances=5):\n",
    "    create_new_db=True\n",
    "    # Initialize databases for CL(4) and CL(n) filtrations\n",
    "    db4 = CLFiltrationDB(filepath_generator(dirname=DIR_NAME,filename=f\"{DB_NAME}_4\", extension='db',overwrite=not create_new_db),create_new_db=create_new_db)\n",
    "    dbn = CLFiltrationDB(filepath_generator(dirname=DIR_NAME,filename=f\"{DB_NAME}_n\", extension='db',overwrite=not create_new_db),create_new_db=create_new_db)\n",
    "    # Extract parameters from configuration file\n",
    "    ladder_len_min = POINT_CLOUD_PARAMS['ladder_len_min']\n",
    "    ladder_len_max = POINT_CLOUD_PARAMS['ladder_len_max']\n",
    "    pts_min = POINT_CLOUD_PARAMS['pts_min']\n",
    "    pts_max = POINT_CLOUD_PARAMS['pts_max']\n",
    "    enable_multi_processing = POINT_CLOUD_PARAMS['enable_multi_processing']\n",
    "    num_cores = POINT_CLOUD_PARAMS['num_cores']\n",
    "    verbose = POINT_CLOUD_PARAMS['verbose']\n",
    "    i=0\n",
    "    while i<num_instances: # Loop through the specified number of instances\n",
    "        num_pts=random.randint(pts_min,pts_max) # Determine the number of points for this model\n",
    "        # Initialize a random filtration\n",
    "        result=RandomFiltrationPointCloudModel(num_pts=num_pts,ladder_length_min=ladder_len_min,ladder_length_max=ladder_len_max,enable_multi_processing=enable_multi_processing,num_cores=num_cores,verbose=verbose)\n",
    "        i+=1\n",
    "        print(i)\n",
    "        if filtration:=result.output[\"CL(4)\"]:\n",
    "            decomp = filtration.info['decomp']\n",
    "            # Identify non-intervals in the decomposition\n",
    "            non_intervals = {k:v for k,v in decomp.items() if k[0] == 'N'}\n",
    "            if non_intervals:\n",
    "                print(f'non_intervals found @ {i}',flush=True)\n",
    "            db4.add_filtration(filtration)\n",
    "        if filtration:=result.output[\"CL(n)\"]:\n",
    "            non_intervals=(pd.read_csv(StringIO(filtration.info['lines']),index_col=0).multiplicity<0).any()\n",
    "            dbn.add_filtration(filtration) # Add CL(n) filtration results to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate five filtrations and store them in the database\n",
    "generate_filtration_point_cloud_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the db_conversion_utils module within the commutazzio.random package\n",
    "from commutazzio.random.db_conversion_utils import sqlite_to_df_cl4_pc, sqlite_to_df_cln_pc, collect_data_from_db_files\n",
    "\n",
    "# Specify the directory containing the database files for processing\n",
    "dirnames = [\"./tutorial_databases/\"]\n",
    "\n",
    "# Define the filename patterns for identifying the specific database files to process\n",
    "fn_pattern_cl4 = \"PointCloudModel_tutorial_4\" \n",
    "fn_pattern_cln = \"PointCloudModel_tutorial_n\"\n",
    "\n",
    "# Use the collect_data_from_db_files function to search within the specified directory (or directories)\n",
    "# for database files matching the given filename patterns. Each matching file's data is then processed\n",
    "# by the respective conversion function (sqlite_to_df_cl4_pc or sqlite_to_df_cln_pc) to create a DataFrame\n",
    "# containing structured data extracted from the databases.\n",
    "df_cl4_pcm = collect_data_from_db_files(dirnames, fn_pattern_cl4, sqlite_to_df_cl4_pc)  # Collect and convert CL4 data\n",
    "df_cln_pcm = collect_data_from_db_files(dirnames, fn_pattern_cln, sqlite_to_df_cln_pc)  # Collect and convert CLn data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
